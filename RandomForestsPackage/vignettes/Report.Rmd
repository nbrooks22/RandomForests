---
title: "Einleitung CART"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Report}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(rmarkdown.html_vignette.check_title = FALSE)
options(tibble.print_min = 10L, tibble.print_max = 10L)
```

```{r setup}
library(RandomForestsPackage)
library(tidyverse)
library(rlang)
set.seed(10)
```

## Einleitung

Das Ziel dieses Paketes ist es mit Hilfe von Entscheidungsbäumen möglichst genaue Vorhersagen aufgrund von gegebenen Beobachtungen zu treffen. Es werden dabei Klassifikations- und Regressionsbäume (CARTs) betrachtet. Das Ziel ist es also die Klasse/den Wert aufgrund von Beobachtungen möglichst genau vorherzusagen (wie zum Beispiel bei einer Regressionsgeraden).

## Daten

Um einen Regressionsbaum zu erstellen, werden folgende Trainingsdaten benutzt (in zwei Formaten)

```{r}
X <- runif(50,0,1)
e <- rnorm(50,0,0.2)
Y <- sin(2*pi*X) + e
data_reg <- tibble(x = X, y = Y)
data_reg2 <- list(x = matrix(X, nrow = 1), y = Y)
```

Für den Klassifikationsbaum haben wir folgende Daten (in zwei Formaten)

```{r}
X1 <- runif(50,0,1)
X2 <- runif(50,0,1)
e1 <- rnorm(50,0,0.2)
kappa <- function(x,y) y - 0.5 - 0.3*sin(2*pi*x)
f <- function(x,y,e){
  Y <- c()
  for(i in seq_along(x)){
    if(kappa(X1[i],X2[i]) - e[i] <= 0){
      Y[i] <- 1
    } else{
      Y[i] <- 2
    }
  }
  Y
}
data_class <- tibble(x1 = X1, x2 = X2, y = f(X1,X2,e1))
data_class2 <- list(x = matrix(c(X1,X2),nrow = 2, byrow = TRUE), y = f(X1,X2,e1))
```

## Funktionen

Es gibt verschiedene Funktionen um Entscheidungsbäume zu erstellen:

-   `greedy_cart()` erstellt einen Baum mittels gierigem Verfahren
-   `pruning()` gibt für einen bereits erstellten Baum (CART) einen Teilbaum zurück. Dieser ist bezüglich des Risikos und eines Parameters lambda, welche die Blattanzahl gewichtet, optimiert.
-   `bagging()` erstellt eine Liste von Bäumen mit Bootstrap Aggregation
-   `random_forest()` erstellt einen Random Forest

Für die Entscheidungsregel/die Schätzung eines Wertes gibt es die folgende Funktion:

-   `make_prediction()`

Funktionen um einen Entscheidungsbaum zu visualisieren:

-   `printRegression()`
-   `printClassification()`
-   `plotTree()`

### Erstelle einen Baum mit `greedy_cart()`

`greedy_cart()` erstellt einen Entscheidungsbaum mit Hilfe des gierigen Verfahrens. Die ersten zwei Argumente geben die Namen der Spalten/Listenelemente an, die man benutzen möchte. Das `data` Argument gibt an, aus welchem Tibble/Liste man die Daten nimmt.

```{r}
data <- greedy_cart(x = x, y = y, data = data_reg, type = "reg")
```

`data` ist eine Umgebung, wobei `data$tree` den Entscheidungsbaum ausgibt, und `data$values` die verwendeten Daten sind. Die Form des Tibbles `data$tree` ist in `?greedy_cart` genauer erklärt. `data$dim` ist die Dimension der verwendeten Daten.

```{r}
data$tree
data$values
data$dim
```

#### Abbruchbedingungen

Wenn man keine Abbruchbedingung angibt, erhält man einen Baum, der in jedem Blatt nur noch einen Datenpunkt besitzt.

##### `num_leaf`

Es wird ein Baum mit insgesamt `num_leaf` Blättern erstellt. Hier wird ein Baum mit 10 Blättern erstellt.

```{r, eval = FALSE}
greedy_cart(x = x, y = y, data = data_reg, type = "reg", num_leaf = 10)
```

##### `depth`

Der Baum wird nur bis zur Tiefe `depth` bestimmt. In diesem Fall bricht der Algorithmus ab, wenn der Baum die Tiefe 2 hat.

```{r, eval = FALSE}
greedy_cart(x = x, y = y, data = data_reg, type = "reg", depth = 2)
```

##### `num_split`

Die minimale Anzahl an Trainingsdaten die in einem Knoten sein soll, damit der Knoten weiter aufgeteilt wird, beträgt `num_split`. Bei 10 Trainingsdaten in einem Knoten wird noch weiter aufgeteilt, bei 9 und weniger nicht mehr.

```{r}
data_1 <- greedy_cart(x = x, y = y, data = data_reg, type = "reg", num_split = 10)
```

##### `min_num`

Man teilt einen Knoten nur dann weiter auf, wenn beide darauffolgenden Blätter aus mindestens `min_num` Elementen bestehen, hier 5 Elemente.

```{r, eval = FALSE}
greedy_cart(x = x, y = y, data = data_reg, type = "reg", min_num = 5)
```

#### Anzeigen eines Baumes mit `plotTree()`

Den Entscheidungsbaum kann man mit der Funktion `plotTree()` zeichnen lassen. Hierbei kann das erste Argument eine Umgebung wie man sie bei `greedy_cart()` bekommt sein, oder eine benannte Liste mit einem Baum in Form eines Tibbles und dem Namen tree.

```{r}
plotTree(data_1)
```

#### Plotten des Entscheidungsbaumes mit `printRegression()`

Man kann sich die entstandene Schätzung eines Regressionsbaumes mit der Funktion `printRegression()` anzeigen lassen. Das erste Argument ist eine Umgebung wie man sie bei `greedy_cart()` bekommt. Das zweite Argument ist der Name des Plots.

```{r, fig.width = 7, fig.height = 5}
printRegression(data_1, "regression")
#make_prediction(list(data_1$tree), matrix(0) ,type = "reg")
```

Analog funktioniert das gierige Verfahren für Klassifikationsprobleme.

```{r}
val <- greedy_cart(x = c(x1, x2), y = y, data = data_class, type = "class", depth = 3)
val$tree
```

Bei dem Klassifikationsverfahren kommt noch eine weitere Abbruchbedingung hinzu:

##### `unique`

Wenn sich nur noch Daten einer Klasse in einem Knoten befinden, wird dieser Knoten nicht weiter aufgeteilt.

```{r}
tree <- greedy_cart(x = c(x1,x2), y = y, data = data_class, type = "class", unique = TRUE)
```

#### Plotten des Entscheidungsbaumes mit `printClassification()`

Die Entscheidungsregel eines Klassifikationsbaumes kann man sich ebenfalls anzeigen lassen mit `printClassification()`. Die beiden Argumente sind wie bei `printRegression()`

```{r}
printClassification(tree, "classification")
```

### Wähle mittels `pruning()` einen bezüglich des Parameters `lambda` optimierten Teilbaum zu einem bestehendem CART

`pruning()` nimmt einen Entscheidungsbaum (CART), einen Parameter `lambda` und einen Typ entgegen und gibt einen optimalen Teilbaum aus. Bei `type` handelt es sich um die Art des Verfahrens. Hierbei steht `type` = "reg" für Regression und `type` ="class" für Klassifikation. Der Parameter `lambda` gewichtet im Term über den optimiert wird die Anzahl der Blätter des jeweiligen Baums.

```{r}
lambda <- 1/100
pruning(data$tree,lambda,type="reg")
```

### Bestimmung des Parameters `lambda` mittels Cross-Validation `cross_validation`

Die Funktion `cross_validation` nimmt Trainingsdaten `data`, eine Parametermenge `Lambda`, eine natürliche Zahl `m` und den Typ `type` entgegen. Bei Typ handelt es sich wieder um die Unterscheidung zwischen Regression (`type`=reg) und Klassifikation (`type`=class). Die natürliche Zahl `m` gibt an mit wievielen Teilmengen eine Partition der Trainingsdaten `data` erstellt werden soll. Der Rückgabewert der Funktion ist die Kompenente des Parametervektors `Lambda`, welche mittels Cross-Validation als bester Schätzer `lambda` für das Pruning-Verfahren geeignet ist. ACHTUNG: Schon für kleines `m` (m \<= 4) und wenige Komponenten in `Lambda` (4 oder weniger Einträge) können hier sehr lange Laufzeiten entstehen.

```{r}
Lambda <- c(1,2)/100
cross_validation(data_reg2, Lambda, m=2, type="reg")
```

### Erstelle Bäume durch Bootstrap Aggregation mit `bagging()`

`bagging()` ist eine Erweiterung von `greedy_cart()`, die mithilfe von Bootstrap Aggregation mehrere Trainingsbäume erstellt, um bessere Vorhersagen treffen zu können. Wir wollen bei jedem Aufruf der Funktion `b` = 3 verschiedene Bäume erstellen.

```{r, results='hide'}
data <- bagging(data = data_reg2, b = 3, type = "reg")
```

`data` ist eine Liste, wobei `data$Bagged_Trees` die Liste der Entscheidungsbäume (jeweils im Format von den `greedy_cart()` returns) ausgibt, und `data$Prediction` ein Vektor von Schätzungen ist. Die Anzahl von ausgegebenen Schätzungen ist gleich der Anzahl von eingegebenen Schätzdaten.

```{r}
data$Bagged_Trees
data$Prediction
```

#### `pred_val`

Wenn man nun gleichzeitig Werte anhand der Beobachtungen schätzen will, kann man eine Matrix als Argument übergeben, wobei jede Spalte einen Vektor von gleicher Dimension wie die x-Werte der Eingangsdaten darstellt (in diesem Fall wollen wir eine Schätzung für 3 verschiedene eindimensionale Werte).

```{r}
pred_matrix <- matrix(c(0.3, 0.5, 1), ncol = 3)
data <- bagging(data = data_reg2, type = "reg", pred_val = pred_matrix)
data$Prediction
```

Analog funktioniert das bagging Verfahren für Klassifikationsprobleme.

```{r}
bagging(data = data_class2, b = 8, type = "class")
```

### `random_forest()`

`random_forest()` ist eine Verallgemeinerung des `bagging` Algorithmus. Wir wollen in jedem Iterationsschritt `m` = 1 Koordinaten benutzen, insgesamt `B` = 5 Bäume erstellen und für jeden Baum `A` = 25 zufällig ausgewählte Daten aus den originalen Daten verwenden. Die Abbruchbedingungen sind die gleichen wie bei `greedy_cart()`. Hier wird solange weitergemacht, bis der Baum `num_leaf` = 15 Blätter hat.

```{r}
random_forest(x = c(x1,x2), y = y, data = data_class, type = "class", B = 5, A = 25, m = 1, num_leaf = 15)
```

### `make_prediction()`

`make_prediction()` ist eine Funktion die uns erlaubt, unsere erstellten Bäume auch zu benutzen. Wir wollen nun eine Schätzung von Werten anhand der Entscheidungsbäume finden. Es wird als erstes Argument eine Liste von Bäumen in Tibbleform übergeben. ACHTUNG: Muss im list() Format übergeben werden, auch wenn es nur ein Baum ist. Das zweite Argument ist eine Matrix mit den Werten die man schätzen will, wobei jede Spalte einen Vektor von gleicher Dimension wie die x-Werte der Eingangsdaten darstellt (in diesem Fall testen wir 4 verschiedene eindimensionale Werte). Das Argument `type` ist wieder entweder `type` = "reg" oder `type` = "class", was auch immer für einen Typ die entsprechenden Bäume haben.

```{r}
treelist <- bagging(data = data_reg2, b = 3, type = "reg")
xlist <- matrix(c(0, 0.25, 0.5, 1), nrow = 1)
make_prediction(treelist$Bagged_Trees, xlist, type = "reg")
```
