---
title: "Einleitung CART"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Report}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(rmarkdown.html_vignette.check_title = FALSE)
options(tibble.print_min = 10L, tibble.print_max = 10L)
```

```{r setup}
library(RandomForestsPackage)
library(tidyverse)
library(rlang)
set.seed(10)
```
## Einleitung
Das Ziel dieses Paketes ist es mit Hilfe von Entscheidungsbäumen möglichst genaue Vorhersagen aufgrund von gegebenen Beobachtungen zu treffen. Es werden dabei Klassifikations- und Regressionsbäume (CARTs) betrachtet. Das Ziel ist es also die Klasse/den Wert aufgrund von Beobachtungen möglichst genau vorherzusagen (wie zum Beispiel bei einer Regressionsgeraden).


## Daten
Um einen Regressionsbaum zu erstellen, werden folgende Trainingsdaten benutzt
```{r}
X <- runif(100,0,1)
e <- rnorm(100,0,0.2)
Y <- sin(2*pi*X) + e
data_reg <- tibble(x = X, y = Y)
```
Für den Klassifikationsbaum haben wir folgende Daten
```{r}
X1 <- runif(100,0,1)
X2 <- runif(100,0,1)
e1 <- rnorm(100,0,0.2)
kappa <- function(x,y) y - 0.5 - 0.3*sin(2*pi*x)
f <- function(x,y,e){
  Y <- c()
  for(i in seq_along(x)){
    if(kappa(X1[i],X2[i]) - e[i] <= 0){
      Y[i] <- 1
    } else{
      Y[i] <- 2
    }
  }
  Y
}
data_class <- tibble(x1 = X1, x2 = X2, y = f(X1,X2,e1))
```



## Funktionen
Es gibt verschiedene Funktionen um Entscheidungsbäume zu erstellen:

* `greedy_cart()` erstellt einen Baum mittels gierigem Verfahren
* `pruning()` gibt für einen bereits erstellten Baum (CART) einen Teilbaum zurück. Dieser ist bezüglich des Risikos und eines Parameters lambda, welche die Blattanzahl gewichtet, optimiert. 
* `bagging()`
* `random_forest()` erstellt einen Random Forest

Für die Entscheidungsregel/die Schätzung eines Wertes gibt es die folgende Funktion:

* `make_prediction()`

Funktionen um einen Entscheidungsbaum zu visualisieren:

* `printGreedyCartRegression()`
* `plotTree()`


### Erstelle einen Baum mit `greedy_cart()`
`greedy_cart()` erstellt einen Entscheidungsbaum mit Hilfe des gierigen Verfahrens. Die ersten zwei Argumente geben die Namen der Spalten/Listenelemente an, die man benutzen möchte. Das `data` Argument gibt an, aus welchem Tibble/Liste man die Daten nimmt.
```{r}
data <- greedy_cart(x = x, y = y, data = data_reg, type = "reg")
```
`data` ist eine Umgebung, wobei `data$tree` den Entscheidungsbaum ausgibt, und `data$values` die verwendeten Daten sind. Die Form des Tibbles `data$tree` ist in `?greedy_cart` genauer erklärt.
`data$dim` ist die Dimension der verwendeten Daten.
```{r}
data$tree
data$values
data$dim
```


#### Abbruchbedingungen
Wenn man keine Abbruchbedingung angibt, erhält man einen Baum, der in jedem Blatt nur noch einen Datenpunkt besitzt.

##### `num_leaf`
Es wird ein Baum mit insgesamt `num_leaf` Blättern erstellt. Hier wird ein Baum mit 10 Blättern erstellt.
```{r, eval = FALSE}
greedy_cart(x = x, y = y, data = data_reg, type = "reg", num_leaf = 10)
```

##### `depth`
Der Baum wird nur bis zur Tiefe `depth` bestimmt. In diesem Fall bricht der Algorithmus ab, wenn der Baum die Tiefe 2 hat.
```{r, eval = FALSE}
greedy_cart(x = x, y = y, data = data_reg, type = "reg", depth = 2)
```
##### `num_split`
Die minimale Anzahl an Trainingsdaten die in einem Knoten sein soll, damit der Knoten weiter aufgeteilt wird, beträgt `num_split`. Bei 10 Trainingsdaten in einem Knoten wird noch weiter aufgeteilt, bei 9 und weniger nicht mehr.
```{r, eval = FALSE}
greedy_cart(x = x, y = y, data = data_reg, type = "reg", num_split = 10)
```
##### `min_num`
Man teilt einen Knoten nur dann weiter auf, wenn beide darauffolgenden Blätter aus mindestens `min_num` Elementen bestehen, hier 5 Elemente.
```{r, eval = FALSE}
greedy_cart(x = x, y = y, data = data_reg, type = "reg", min_num = 5)
```

Analog funktioniert das gierige Verfahren für Klassifikationsprobleme.
```{r}
val <- greedy_cart(x = c(x1, x2), y = y, data = data_class, type = "class", depth = 3)
val$tree
```

### Wähle mittels `pruning()` einen bezüglich des Parameters lambda optmierten Teilbaum zu einem bestehendem CART
`pruning()` nimmt einen Entscheidungsbaum (CART), einen Parameter `lambda` und einen Typ entgegen. Bei `type` handelt es sich um die Art des Verfahrens. Hierbei steht
`type` = "reg" für Regression und `type` ="class" für Klassifikation. Der Parameter `lambda` gewichtet im Term über den optimiert wird die Anzahl der Blätter des jeweiligen Baums. 
``` {r}
data <- greedy_cart_regression(create_random_sample_data_reg(10,10))
lambda <- 1/100
pruning(data$tree,lambda,type="reg")
```

### Bestimmung es Parameters `lambda` mittels Cross-Validation `cross_validation`
Die Funktion `cross_validation` nimmt Trainingsdaten `data`, eine Parametermenge `Lambda`, eine natürliche Zahl `m` und den Typ `type` entgegen. Bei Typ handelt es sich wieder um die Unterscheidung zwischen Regression (`type`=reg) und KLassifikation (`type`=class). Die natürliche Zahl `m` gibt mit wievielen Teilmengen eine Partition der Trainingsdaten `data` erstellt werden soll. Der Rückgabewert der Funktion ist die Kompenente des Parametervektors `Lambda`, welche mittels Cross-Validation als bester Schätzer `lambda` für das Pruning-Verfahren geeignet ist. 
ACHTUNG: Schon für kleines `m` (m <= 4) und wenige Komponenten in `Lambda` (4 oder weniger Einträge) können hier sehr lange Laufzeiten entstehen. 

``` {r}
data <- create_random_sample_data_reg(10,10)
Lambda <- c(1,2,3)/100
cross_validation(data, Lambda, m=3, type="reg")

```

### `bagging()`

### `random_forest()`
`random_forest()` ist eine Verallgemeinerung des `bagging` Algorithmus. Wir wollen in jedem Iterationsschritt `m` = 1 Koordinaten benutzen, insgesamt `B` = 5 Bäume erstellen und für jeden Baum `A` = 50 zufällig ausgewählte Daten aus den originalen Daten verwenden. Die Abbruchbedingungen sind die gleichen wie bei `greedy_cart()`. Hier wird solange weitergemacht, bis der Baum `num_leaf` = 25 Blätter hat.
```{r}
data <- random_forest(x = c(x1,x2), y = y, data = data_class, type = "class", B = 5, A = 50, m = 1, num_leaf = 25)
```
Die verschiedenen Bäume sehen wie folgt aus:
```{r, echo = FALSE}
for(i in seq_along(data)) print(data[[i]]$tree, n = 5)
```



